{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE TOKENIZER CLASS IS BASICALLY USED TO DO PREPROCESSING LIKE TOKENOZING (DETECTING UNIQUE WORDS), STOP WORDS REMOVAL, REGEX(SPECIAL CHARACTERS REMOVAL), REMOVING ARTICLES (SINCE THEY ARE ABUNDANT AND DO NOT ADD TO THE MEANING OF THE FILE), REMOVING EXCESS WHITE SPACE, APOSTROPHES.\n",
    "\n",
    "NOTE: SINCE NLP BUILT LIBRARIES ARE NOT ALLOWED I HAVE NOT USED LEMMETIZATION TO FIND LEMMAS OF SIMILAR WORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenizer:\n",
    "    def tokenize(self,sentences):\n",
    "        for i in sentences:\n",
    "            tokens = sentences.split()\n",
    "            return tokens\n",
    "    \n",
    "    def stop_words_removal(self,sentences):\n",
    "        new_text=\"\"\n",
    "        tokens=[i.lower() for i in sentences]\n",
    "        list_stop_words=['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']\n",
    "        stop_words_removed_string=[word for word in tokens if word not in list_stop_words]\n",
    "        for i in stop_words_removed_string:\n",
    "            new_text=new_text+\" \"+i\n",
    "        return new_text\n",
    "    \n",
    "    def regex1(self,sentences):\n",
    "        removeSpecialChars = sentences.translate ({ord(c): \" \" for c in \"!@#$%^&*()[]{};:,./<>?\\|`~-=_+\"})\n",
    "        return removeSpecialChars\n",
    "    \n",
    "    def remove_articles(self,sentences):\n",
    "        removeArticles = re.sub('\\s+(a|an|and|the)(\\s+)', '\\2', sentences)\n",
    "        return removeArticles\n",
    "\n",
    "    def excess_white_space_removal(self,sentences):\n",
    "        return \" \".join(sentences.split())\n",
    "    \n",
    "    def apostrophes(self,sentences):\n",
    "        return sentences.replace(\"'\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW WE FIRST MAKE A count_terms_dictionary(DICTIONARY CONTAINING UNIQUE TERMS ALONG WITH ITS FREQUENCY OF OCCURANCE), computeTF(COMPUTES THE FREQUENCY OF EACH TERM WITH RESPECT TO LENGTH OF THE STRING), computeIDF(COMPUTES WHETHER THE TERM IS COMMON OR RARE IN A GIVEN DOCUMENT), computeTFIDF (BASICALLY MULTIPLIES TERM FREQUENCY WITH INVERSE DOCUMENT FREQUENCY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tf_idf1:\n",
    "    def count_terms_dictionary(self,sentences):\n",
    "        DF={}\n",
    "        words=sentences.split()\n",
    "        for i in range(len(words)):\n",
    "            if words[i] not in DF:\n",
    "                DF[words[i]]=1\n",
    "            else:\n",
    "                DF[words[i]]+=1\n",
    "        return DF\n",
    "        \n",
    "    def computeTF(self,sentences):\n",
    "        tfDict={}\n",
    "        s1=tf_idf1()\n",
    "        counter=s1.count_terms_dictionary(sentences)\n",
    "        words_count=len(sentences.split())\n",
    "        for w,c in counter.items():\n",
    "            tfDict[w]=c/float(words_count)\n",
    "        return tfDict\n",
    "    \n",
    "    def computeIDF(self,sentences):\n",
    "        s1=tf_idf1()\n",
    "        idfDict={}\n",
    "        N=len(sentences.split())\n",
    "        idfDict=s1.computeTF(sentences)\n",
    "        for w,c in idfDict.items():\n",
    "            idfDict[w] = math.log10(N/(float(c)+1))\n",
    "            \n",
    "        return idfDict\n",
    "    \n",
    "    def computeTFIDF(self,sentences):\n",
    "        s1=tf_idf1()\n",
    "        tfidf={}\n",
    "        counter=s1.computeTF(sentences)\n",
    "        idfs=s1.computeIDF(sentences)\n",
    "        for w,c in counter.items():\n",
    "            tfidf[w]=c*idfs[w]\n",
    "        return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CALCULATE IS BASICALLY A PIPELINE FOR CALLING ALL THE ABOVE FUNCTIONS IN ORDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "class calc:\n",
    "    def calculate(self,sent):\n",
    "        s=tokenizer()\n",
    "        s1=tf_idf1()\n",
    "        sent1=s.tokenize(sent)\n",
    "        sent2=s.stop_words_removal(sent1)\n",
    "        sent3=s.regex1(sent2)\n",
    "        sent4=s.remove_articles(sent3)\n",
    "        sent5=s.excess_white_space_removal(sent4)\n",
    "        sent6=s.apostrophes(sent5)\n",
    "        tfidfs=s1.computeTFIDF(sent6)\n",
    "        return tfidfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE SCORE FUNCTION IS USED TO SCORE THE DOCUMENTS BASED ON COSINE DISTANCE. THE SCORE EXISTS BETWEEN 0 AND 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scoring:\n",
    "    def score(self,a,b):\n",
    "        c=pd.DataFrame([a,b])\n",
    "        k=c.fillna(0)\n",
    "        k1=k.iloc[0]\n",
    "        k2=k.iloc[1]\n",
    "        dot_prod=np.dot(k1,k2)\n",
    "        squares1=0\n",
    "        squares2=0\n",
    "        squares1=[i*i for i in k1]\n",
    "        sum_squares1=sum(squares1)\n",
    "        mag_squares1=np.sqrt(sum_squares1)\n",
    "        squares2=[i*i for i in k2]\n",
    "        sum_squares2=sum(squares2)\n",
    "        mag_squares2=np.sqrt(sum_squares2)\n",
    "        cosine=dot_prod/(mag_squares1*mag_squares2)\n",
    "        return cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE MAIN FUNCTION IS USED TO CALCULATE THE SCORES FOR TWO DIFFERENT TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "class main_function:\n",
    "    def run(self,text1,text2):\n",
    "        t1=calc()\n",
    "        t2=scoring()\n",
    "        tfidf_1=t1.calculate(text1)\n",
    "        tfidf_2=t1.calculate(text2)\n",
    "        scores=t2.score(tfidf_1,tfidf_2)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=\"The easiest way to earn points with Fetch Rewards is to just shop for the products you already love. If you have any participating brands on your receipt, you'll get points based on the cost of the products. You don't need to clip any coupons or scan individual barcodes. Just scan each grocery receipt after you shop and we'll find the savings for you.\"\n",
    "text2=\"The easiest way to earn points with Fetch Rewards is to just shop for the items you already buy. If you have any eligible brands on your receipt, you will get points based on the total cost of the products. You do not need to cut out any coupons or scan individual UPCs. Just scan your receipt after you check out and we will find the savings for you.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7924320151575729\n"
     ]
    }
   ],
   "source": [
    "l1=main_function()\n",
    "final_score=l1.run(text1,text2)\n",
    "print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
